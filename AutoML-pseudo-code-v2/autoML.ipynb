{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "##### FY 2023 HHS Contingency Staffing Plan for a Lapse in Appropriation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>staff_involved</th>\n",
       "      <th>acf</th>\n",
       "      <th>acl</th>\n",
       "      <th>ahrq</th>\n",
       "      <th>aspr</th>\n",
       "      <th>cdc</th>\n",
       "      <th>cms</th>\n",
       "      <th>fda</th>\n",
       "      <th>hrsa</th>\n",
       "      <th>ihs</th>\n",
       "      <th>nih</th>\n",
       "      <th>os</th>\n",
       "      <th>samhsa</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Staff normally paid from or shifted to adminis...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>1662.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>792.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1655.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4246.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staff normally paid from or shifted to carryov...</td>\n",
       "      <td>639.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>428.00</td>\n",
       "      <td>2303.00</td>\n",
       "      <td>525.00</td>\n",
       "      <td>11714.00</td>\n",
       "      <td>523.00</td>\n",
       "      <td>258.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>1717.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18358.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Staff normally paid from or shifted to reimbur...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>137.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11766.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11991.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Commissioned Corps (excepted) /1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>643.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>245.00</td>\n",
       "      <td>73.00</td>\n",
       "      <td>1524.00</td>\n",
       "      <td>193.00</td>\n",
       "      <td>228.00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3173.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HHS officers appointed by the President (exempt)</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Activities required to ensure that fully funde...</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>669.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>268.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>415.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1922.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Law enforcement activities.</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Orderly phase-down and suspension of operations</td>\n",
       "      <td>19.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>337.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>255.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1045.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Other</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Staff to be furloughed.</td>\n",
       "      <td>719.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7693.00</td>\n",
       "      <td>3297.00</td>\n",
       "      <td>3595.00</td>\n",
       "      <td>1282.00</td>\n",
       "      <td>554.00</td>\n",
       "      <td>14687.00</td>\n",
       "      <td>2264.00</td>\n",
       "      <td>574.0</td>\n",
       "      <td>35074.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Subtotal, authorized by law</td>\n",
       "      <td>1439.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>276.00</td>\n",
       "      <td>551.00</td>\n",
       "      <td>11899.00</td>\n",
       "      <td>6367.00</td>\n",
       "      <td>15943.00</td>\n",
       "      <td>2837.00</td>\n",
       "      <td>14615.00</td>\n",
       "      <td>15633.00</td>\n",
       "      <td>6361.00</td>\n",
       "      <td>638.0</td>\n",
       "      <td>76738.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Direct medical services provided through clini...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>2219.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2773.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Other</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>362.00</td>\n",
       "      <td>1798.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2830.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>375.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5381.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Maintain computer data</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>148.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Maintenance of animals &amp; protection of inanima...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>623.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>805.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Other</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Protect ongoing medical experiments.</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>607.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>758.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Subtotal, safety of human life and protection ...</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>2151.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2929.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>397.00</td>\n",
       "      <td>3597.00</td>\n",
       "      <td>605.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10237.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Total on board staffing</td>\n",
       "      <td>1455.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>276.00</td>\n",
       "      <td>1069.00</td>\n",
       "      <td>14050.00</td>\n",
       "      <td>6367.00</td>\n",
       "      <td>18872.00</td>\n",
       "      <td>2859.00</td>\n",
       "      <td>15012.00</td>\n",
       "      <td>19230.00</td>\n",
       "      <td>6966.00</td>\n",
       "      <td>640.0</td>\n",
       "      <td>86975.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Total number of staff to be retained</td>\n",
       "      <td>736.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1069.00</td>\n",
       "      <td>6357.00</td>\n",
       "      <td>3070.00</td>\n",
       "      <td>15277.00</td>\n",
       "      <td>1577.00</td>\n",
       "      <td>14458.00</td>\n",
       "      <td>4543.00</td>\n",
       "      <td>4702.00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>51901.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Exempt</td>\n",
       "      <td>641.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>2515.00</td>\n",
       "      <td>2742.00</td>\n",
       "      <td>12058.00</td>\n",
       "      <td>1347.00</td>\n",
       "      <td>12085.00</td>\n",
       "      <td>231.00</td>\n",
       "      <td>3400.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35472.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Excepted</td>\n",
       "      <td>95.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>640.00</td>\n",
       "      <td>3842.00</td>\n",
       "      <td>328.00</td>\n",
       "      <td>3219.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>2373.00</td>\n",
       "      <td>4312.00</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>65.0</td>\n",
       "      <td>16429.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Number of staff to be furloughed /2</td>\n",
       "      <td>719.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7693.00</td>\n",
       "      <td>3297.00</td>\n",
       "      <td>3595.00</td>\n",
       "      <td>1282.00</td>\n",
       "      <td>554.00</td>\n",
       "      <td>14687.00</td>\n",
       "      <td>2264.00</td>\n",
       "      <td>574.0</td>\n",
       "      <td>35074.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Percent Retained</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Percent Exempt</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Percent Excepted</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Percent Furloughed</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Life &amp; Property as % Excepted</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       staff_involved      acf     acl  \\\n",
       "0   Staff normally paid from or shifted to adminis...     1.00    0.00   \n",
       "1   Staff normally paid from or shifted to carryov...   639.00    8.00   \n",
       "2   Staff normally paid from or shifted to reimbur...     0.00    2.00   \n",
       "3                    Commissioned Corps (excepted) /1     5.00    0.00   \n",
       "4    HHS officers appointed by the President (exempt)     1.00    0.00   \n",
       "5   Activities required to ensure that fully funde...    55.00    0.00   \n",
       "6                         Law enforcement activities.     0.00    0.00   \n",
       "7     Orderly phase-down and suspension of operations    19.00   12.00   \n",
       "8                                               Other     0.00    0.00   \n",
       "9                             Staff to be furloughed.   719.00  157.00   \n",
       "10                        Subtotal, authorized by law  1439.00  179.00   \n",
       "11  Direct medical services provided through clini...     0.00    0.00   \n",
       "12                                              Other    14.00    0.00   \n",
       "13                             Maintain computer data     0.00    0.00   \n",
       "14  Maintenance of animals & protection of inanima...     2.00    0.00   \n",
       "15                                              Other     0.00    0.00   \n",
       "16               Protect ongoing medical experiments.     0.00    0.00   \n",
       "17  Subtotal, safety of human life and protection ...    16.00    0.00   \n",
       "18                            Total on board staffing  1455.00  179.00   \n",
       "19               Total number of staff to be retained   736.00   22.00   \n",
       "20                                             Exempt   641.00   10.00   \n",
       "21                                           Excepted    95.00   12.00   \n",
       "22                Number of staff to be furloughed /2   719.00  157.00   \n",
       "23                                   Percent Retained     0.51    0.12   \n",
       "24                                     Percent Exempt     0.44    0.06   \n",
       "25                                   Percent Excepted     0.07    0.07   \n",
       "26                                 Percent Furloughed     0.49    0.88   \n",
       "27                      Life & Property as % Excepted     0.01    0.00   \n",
       "\n",
       "      ahrq     aspr       cdc      cms       fda     hrsa       ihs       nih  \\\n",
       "0     0.00     0.00     75.00  1662.00      0.00   792.00     61.00      0.00   \n",
       "1    13.00   428.00   2303.00   525.00  11714.00   523.00    258.00    230.00   \n",
       "2     0.00     0.00    137.00     0.00     67.00     0.00  11766.00      0.00   \n",
       "3     5.00   118.00    643.00   103.00    245.00    73.00   1524.00    193.00   \n",
       "4     0.00     1.00      0.00     1.00      1.00     0.00      0.00      1.00   \n",
       "5     0.00     0.00    669.00   125.00    268.00    69.00    415.00    104.00   \n",
       "6     0.00     0.00      0.00     0.00     11.00     0.00      0.00     68.00   \n",
       "7     6.00     3.00    337.00   100.00     10.00    66.00     37.00    255.00   \n",
       "8     0.00     1.00     42.00     0.00      0.00     0.00      0.00     95.00   \n",
       "9   252.00     0.00   7693.00  3297.00   3595.00  1282.00    554.00  14687.00   \n",
       "10  276.00   551.00  11899.00  6367.00  15943.00  2837.00  14615.00  15633.00   \n",
       "11    0.00   156.00     32.00     0.00      0.00     7.00    203.00   2219.00   \n",
       "12    0.00   362.00   1798.00     0.00   2830.00     0.00      0.00      0.00   \n",
       "13    0.00     0.00    115.00     0.00      7.00     5.00    157.00    148.00   \n",
       "14    0.00     0.00     80.00     0.00     15.00     0.00     37.00    623.00   \n",
       "15    0.00     0.00     59.00     0.00      0.00     3.00      0.00      0.00   \n",
       "16    0.00     0.00     67.00     0.00     77.00     7.00      0.00    607.00   \n",
       "17    0.00   518.00   2151.00     0.00   2929.00    22.00    397.00   3597.00   \n",
       "18  276.00  1069.00  14050.00  6367.00  18872.00  2859.00  15012.00  19230.00   \n",
       "19   24.00  1069.00   6357.00  3070.00  15277.00  1577.00  14458.00   4543.00   \n",
       "20   13.00   429.00   2515.00  2742.00  12058.00  1347.00  12085.00    231.00   \n",
       "21   11.00   640.00   3842.00   328.00   3219.00   230.00   2373.00   4312.00   \n",
       "22  252.00     0.00   7693.00  3297.00   3595.00  1282.00    554.00  14687.00   \n",
       "23    0.09     1.00      0.45     0.48      0.81     0.55      0.96      0.24   \n",
       "24    0.05     0.40      0.18     0.43      0.64     0.47      0.81      0.01   \n",
       "25    0.04     0.60      0.27     0.05      0.17     0.08      0.16      0.22   \n",
       "26    0.91     0.00      0.55     0.52      0.19     0.45      0.04      0.76   \n",
       "27    0.00     0.48      0.15     0.00      0.16     0.01      0.03      0.19   \n",
       "\n",
       "         os  samhsa      total  \n",
       "0   1655.00     0.0   4246.000  \n",
       "1   1717.00     0.0  18358.000  \n",
       "2     19.00     0.0  11991.000  \n",
       "3    228.00    36.0   3173.000  \n",
       "4      9.00     1.0     15.000  \n",
       "5    217.00     0.0   1922.000  \n",
       "6      0.00     0.0     79.000  \n",
       "7    173.00    27.0   1045.000  \n",
       "8     79.00     0.0    217.000  \n",
       "9   2264.00   574.0  35074.000  \n",
       "10  6361.00   638.0  76738.000  \n",
       "11   156.00     0.0   2773.000  \n",
       "12   375.00     2.0   5381.000  \n",
       "13    26.00     0.0    458.000  \n",
       "14    48.00     0.0    805.000  \n",
       "15     0.00     0.0     62.000  \n",
       "16     0.00     0.0    758.000  \n",
       "17   605.00     2.0  10237.000  \n",
       "18  6966.00   640.0  86975.000  \n",
       "19  4702.00    66.0  51901.000  \n",
       "20  3400.00     1.0  35472.000  \n",
       "21  1302.00    65.0  16429.000  \n",
       "22  2264.00   574.0  35074.000  \n",
       "23     0.67     0.1      0.597  \n",
       "24     0.49     0.0      0.408  \n",
       "25     0.19     0.1      0.189  \n",
       "26     0.33     0.9      0.403  \n",
       "27     0.09     0.0      0.120  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "csp = pd.read_csv('https://healthdata.gov/resource/nqtp-eetp.csv')\n",
    "csp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['staff_involved', 'acf', 'acl', 'ahrq', 'aspr', 'cdc', 'cms', 'fda',\n",
       "       'hrsa', 'ihs', 'nih', 'os', 'samhsa', 'total'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential variables of interest\n",
    "  - CDC\n",
    "  - CMS\n",
    "  - FDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "csp = csp.rename(columns={'staff_involved':'Staff Involved', 'cdc':'Centers for Disease Control and Prevention','cms': 'Centers for Medicare and Medicaid Services', 'fda': 'Food and Drug Administration'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Staff Involved', 'acf', 'acl', 'ahrq', 'aspr',\n",
       "       'Centers for Disease Control and Prevention',\n",
       "       'Centers for Medicare and Medicaid Services',\n",
       "       'Food and Drug Administration', 'hrsa', 'ihs', 'nih', 'os', 'samhsa',\n",
       "       'total'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       28.000000\n",
       "mean      2234.235714\n",
       "std       3816.499022\n",
       "min          0.000000\n",
       "25%         24.137500\n",
       "50%        126.000000\n",
       "75%       2356.000000\n",
       "max      14050.000000\n",
       "Name: Centers for Disease Control and Prevention, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp['Centers for Disease Control and Prevention'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       28.000000\n",
       "mean      3240.534643\n",
       "std       5725.972098\n",
       "min          0.000000\n",
       "25%          0.527500\n",
       "50%         41.000000\n",
       "75%       3313.000000\n",
       "max      18872.000000\n",
       "Name: Food and Drug Administration, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp['Food and Drug Administration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      28.000000\n",
       "mean      999.481429\n",
       "std      1875.494810\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%       809.250000\n",
       "max      6367.000000\n",
       "Name: Centers for Medicare and Medicaid Services, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp['Centers for Medicare and Medicaid Services'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of 0     Staff normally paid from or shifted to adminis...\n",
       "1     Staff normally paid from or shifted to carryov...\n",
       "2     Staff normally paid from or shifted to reimbur...\n",
       "3                      Commissioned Corps (excepted) /1\n",
       "4      HHS officers appointed by the President (exempt)\n",
       "5     Activities required to ensure that fully funde...\n",
       "6                           Law enforcement activities.\n",
       "7       Orderly phase-down and suspension of operations\n",
       "8                                                 Other\n",
       "9                               Staff to be furloughed.\n",
       "10                          Subtotal, authorized by law\n",
       "11    Direct medical services provided through clini...\n",
       "12                                                Other\n",
       "13                               Maintain computer data\n",
       "14    Maintenance of animals & protection of inanima...\n",
       "15                                                Other\n",
       "16                 Protect ongoing medical experiments.\n",
       "17    Subtotal, safety of human life and protection ...\n",
       "18                              Total on board staffing\n",
       "19                 Total number of staff to be retained\n",
       "20                                               Exempt\n",
       "21                                             Excepted\n",
       "22                  Number of staff to be furloughed /2\n",
       "23                                     Percent Retained\n",
       "24                                       Percent Exempt\n",
       "25                                     Percent Excepted\n",
       "26                                   Percent Furloughed\n",
       "27                        Life & Property as % Excepted\n",
       "Name: Staff Involved, dtype: object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp['Staff Involved'].describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = csp[[\"Staff Involved\"]]\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = csp[[\"Centers for Disease Control and Prevention\", \"Centers for Medicare and Medicaid Services\", \"Food and Drug Administration\"]]\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Staff Involved', 'acf', 'acl', 'ahrq', 'aspr',\n",
       "       'Centers for Disease Control and Prevention',\n",
       "       'Centers for Medicare and Medicaid Services',\n",
       "       'Food and Drug Administration', 'hrsa', 'ihs', 'nih', 'os', 'samhsa',\n",
       "       'total'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acf</th>\n",
       "      <th>acl</th>\n",
       "      <th>ahrq</th>\n",
       "      <th>aspr</th>\n",
       "      <th>Centers for Disease Control and Prevention</th>\n",
       "      <th>Centers for Medicare and Medicaid Services</th>\n",
       "      <th>Food and Drug Administration</th>\n",
       "      <th>hrsa</th>\n",
       "      <th>ihs</th>\n",
       "      <th>nih</th>\n",
       "      <th>os</th>\n",
       "      <th>samhsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>1662.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>792.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1655.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>639.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>428.00</td>\n",
       "      <td>2303.00</td>\n",
       "      <td>525.00</td>\n",
       "      <td>11714.00</td>\n",
       "      <td>523.00</td>\n",
       "      <td>258.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>1717.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>137.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11766.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>643.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>245.00</td>\n",
       "      <td>73.00</td>\n",
       "      <td>1524.00</td>\n",
       "      <td>193.00</td>\n",
       "      <td>228.00</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>669.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>268.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>415.00</td>\n",
       "      <td>104.00</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>337.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>255.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>719.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7693.00</td>\n",
       "      <td>3297.00</td>\n",
       "      <td>3595.00</td>\n",
       "      <td>1282.00</td>\n",
       "      <td>554.00</td>\n",
       "      <td>14687.00</td>\n",
       "      <td>2264.00</td>\n",
       "      <td>574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1439.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>276.00</td>\n",
       "      <td>551.00</td>\n",
       "      <td>11899.00</td>\n",
       "      <td>6367.00</td>\n",
       "      <td>15943.00</td>\n",
       "      <td>2837.00</td>\n",
       "      <td>14615.00</td>\n",
       "      <td>15633.00</td>\n",
       "      <td>6361.00</td>\n",
       "      <td>638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>2219.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>362.00</td>\n",
       "      <td>1798.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2830.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>375.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>148.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>623.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>607.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>2151.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2929.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>397.00</td>\n",
       "      <td>3597.00</td>\n",
       "      <td>605.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1455.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>276.00</td>\n",
       "      <td>1069.00</td>\n",
       "      <td>14050.00</td>\n",
       "      <td>6367.00</td>\n",
       "      <td>18872.00</td>\n",
       "      <td>2859.00</td>\n",
       "      <td>15012.00</td>\n",
       "      <td>19230.00</td>\n",
       "      <td>6966.00</td>\n",
       "      <td>640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>736.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1069.00</td>\n",
       "      <td>6357.00</td>\n",
       "      <td>3070.00</td>\n",
       "      <td>15277.00</td>\n",
       "      <td>1577.00</td>\n",
       "      <td>14458.00</td>\n",
       "      <td>4543.00</td>\n",
       "      <td>4702.00</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>641.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>2515.00</td>\n",
       "      <td>2742.00</td>\n",
       "      <td>12058.00</td>\n",
       "      <td>1347.00</td>\n",
       "      <td>12085.00</td>\n",
       "      <td>231.00</td>\n",
       "      <td>3400.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>95.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>640.00</td>\n",
       "      <td>3842.00</td>\n",
       "      <td>328.00</td>\n",
       "      <td>3219.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>2373.00</td>\n",
       "      <td>4312.00</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>719.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7693.00</td>\n",
       "      <td>3297.00</td>\n",
       "      <td>3595.00</td>\n",
       "      <td>1282.00</td>\n",
       "      <td>554.00</td>\n",
       "      <td>14687.00</td>\n",
       "      <td>2264.00</td>\n",
       "      <td>574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acf     acl    ahrq     aspr  \\\n",
       "0      1.00    0.00    0.00     0.00   \n",
       "1    639.00    8.00   13.00   428.00   \n",
       "2      0.00    2.00    0.00     0.00   \n",
       "3      5.00    0.00    5.00   118.00   \n",
       "4      1.00    0.00    0.00     1.00   \n",
       "5     55.00    0.00    0.00     0.00   \n",
       "6      0.00    0.00    0.00     0.00   \n",
       "7     19.00   12.00    6.00     3.00   \n",
       "8      0.00    0.00    0.00     1.00   \n",
       "9    719.00  157.00  252.00     0.00   \n",
       "10  1439.00  179.00  276.00   551.00   \n",
       "11     0.00    0.00    0.00   156.00   \n",
       "12    14.00    0.00    0.00   362.00   \n",
       "13     0.00    0.00    0.00     0.00   \n",
       "14     2.00    0.00    0.00     0.00   \n",
       "15     0.00    0.00    0.00     0.00   \n",
       "16     0.00    0.00    0.00     0.00   \n",
       "17    16.00    0.00    0.00   518.00   \n",
       "18  1455.00  179.00  276.00  1069.00   \n",
       "19   736.00   22.00   24.00  1069.00   \n",
       "20   641.00   10.00   13.00   429.00   \n",
       "21    95.00   12.00   11.00   640.00   \n",
       "22   719.00  157.00  252.00     0.00   \n",
       "23     0.51    0.12    0.09     1.00   \n",
       "24     0.44    0.06    0.05     0.40   \n",
       "25     0.07    0.07    0.04     0.60   \n",
       "26     0.49    0.88    0.91     0.00   \n",
       "27     0.01    0.00    0.00     0.48   \n",
       "\n",
       "    Centers for Disease Control and Prevention  \\\n",
       "0                                        75.00   \n",
       "1                                      2303.00   \n",
       "2                                       137.00   \n",
       "3                                       643.00   \n",
       "4                                         0.00   \n",
       "5                                       669.00   \n",
       "6                                         0.00   \n",
       "7                                       337.00   \n",
       "8                                        42.00   \n",
       "9                                      7693.00   \n",
       "10                                    11899.00   \n",
       "11                                       32.00   \n",
       "12                                     1798.00   \n",
       "13                                      115.00   \n",
       "14                                       80.00   \n",
       "15                                       59.00   \n",
       "16                                       67.00   \n",
       "17                                     2151.00   \n",
       "18                                    14050.00   \n",
       "19                                     6357.00   \n",
       "20                                     2515.00   \n",
       "21                                     3842.00   \n",
       "22                                     7693.00   \n",
       "23                                        0.45   \n",
       "24                                        0.18   \n",
       "25                                        0.27   \n",
       "26                                        0.55   \n",
       "27                                        0.15   \n",
       "\n",
       "    Centers for Medicare and Medicaid Services  Food and Drug Administration  \\\n",
       "0                                      1662.00                          0.00   \n",
       "1                                       525.00                      11714.00   \n",
       "2                                         0.00                         67.00   \n",
       "3                                       103.00                        245.00   \n",
       "4                                         1.00                          1.00   \n",
       "5                                       125.00                        268.00   \n",
       "6                                         0.00                         11.00   \n",
       "7                                       100.00                         10.00   \n",
       "8                                         0.00                          0.00   \n",
       "9                                      3297.00                       3595.00   \n",
       "10                                     6367.00                      15943.00   \n",
       "11                                        0.00                          0.00   \n",
       "12                                        0.00                       2830.00   \n",
       "13                                        0.00                          7.00   \n",
       "14                                        0.00                         15.00   \n",
       "15                                        0.00                          0.00   \n",
       "16                                        0.00                         77.00   \n",
       "17                                        0.00                       2929.00   \n",
       "18                                     6367.00                      18872.00   \n",
       "19                                     3070.00                      15277.00   \n",
       "20                                     2742.00                      12058.00   \n",
       "21                                      328.00                       3219.00   \n",
       "22                                     3297.00                       3595.00   \n",
       "23                                        0.48                          0.81   \n",
       "24                                        0.43                          0.64   \n",
       "25                                        0.05                          0.17   \n",
       "26                                        0.52                          0.19   \n",
       "27                                        0.00                          0.16   \n",
       "\n",
       "       hrsa       ihs       nih       os  samhsa  \n",
       "0    792.00     61.00      0.00  1655.00     0.0  \n",
       "1    523.00    258.00    230.00  1717.00     0.0  \n",
       "2      0.00  11766.00      0.00    19.00     0.0  \n",
       "3     73.00   1524.00    193.00   228.00    36.0  \n",
       "4      0.00      0.00      1.00     9.00     1.0  \n",
       "5     69.00    415.00    104.00   217.00     0.0  \n",
       "6      0.00      0.00     68.00     0.00     0.0  \n",
       "7     66.00     37.00    255.00   173.00    27.0  \n",
       "8      0.00      0.00     95.00    79.00     0.0  \n",
       "9   1282.00    554.00  14687.00  2264.00   574.0  \n",
       "10  2837.00  14615.00  15633.00  6361.00   638.0  \n",
       "11     7.00    203.00   2219.00   156.00     0.0  \n",
       "12     0.00      0.00      0.00   375.00     2.0  \n",
       "13     5.00    157.00    148.00    26.00     0.0  \n",
       "14     0.00     37.00    623.00    48.00     0.0  \n",
       "15     3.00      0.00      0.00     0.00     0.0  \n",
       "16     7.00      0.00    607.00     0.00     0.0  \n",
       "17    22.00    397.00   3597.00   605.00     2.0  \n",
       "18  2859.00  15012.00  19230.00  6966.00   640.0  \n",
       "19  1577.00  14458.00   4543.00  4702.00    66.0  \n",
       "20  1347.00  12085.00    231.00  3400.00     1.0  \n",
       "21   230.00   2373.00   4312.00  1302.00    65.0  \n",
       "22  1282.00    554.00  14687.00  2264.00   574.0  \n",
       "23     0.55      0.96      0.24     0.67     0.1  \n",
       "24     0.47      0.81      0.01     0.49     0.0  \n",
       "25     0.08      0.16      0.22     0.19     0.1  \n",
       "26     0.45      0.04      0.76     0.33     0.9  \n",
       "27     0.01      0.03      0.19     0.09     0.0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = csp[['acf', 'acl', 'ahrq', 'aspr',\n",
    "       'Centers for Disease Control and Prevention',\n",
    "       'Centers for Medicare and Medicaid Services',\n",
    "       'Food and Drug Administration', 'hrsa', 'ihs', 'nih', 'os', 'samhsa']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4246.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18358.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11991.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3173.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1922.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>79.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1045.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>217.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35074.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>76738.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2773.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5381.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>458.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>805.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>758.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10237.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>86975.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51901.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35472.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16429.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>35074.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total\n",
       "0    4246.000\n",
       "1   18358.000\n",
       "2   11991.000\n",
       "3    3173.000\n",
       "4      15.000\n",
       "5    1922.000\n",
       "6      79.000\n",
       "7    1045.000\n",
       "8     217.000\n",
       "9   35074.000\n",
       "10  76738.000\n",
       "11   2773.000\n",
       "12   5381.000\n",
       "13    458.000\n",
       "14    805.000\n",
       "15     62.000\n",
       "16    758.000\n",
       "17  10237.000\n",
       "18  86975.000\n",
       "19  51901.000\n",
       "20  35472.000\n",
       "21  16429.000\n",
       "22  35074.000\n",
       "23      0.597\n",
       "24      0.408\n",
       "25      0.189\n",
       "26      0.403\n",
       "27      0.120"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y =csp[['total']]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_8\n",
      "The task is regression with evaluation metric rmse\n",
      "AutoML will use algorithms: ['Baseline', 'Linear', 'Decision Tree', 'Random Forest', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "1_Baseline rmse 33250.630161 trained in 1.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disable SHAP explanations because of small number of samples (< 20).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_DecisionTree rmse 4730.421495 trained in 1.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "Disable SHAP explanations because of small number of samples (< 20).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_Linear rmse 5054601369.395733 trained in 0.33 seconds\n",
      "* Step default_algorithms will try to check up to 3 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "Disable SHAP explanations because of small number of samples (< 20).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_Default_Xgboost rmse 32648.65121 trained in 0.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_Default_NeuralNetwork rmse 31331.132437 trained in 0.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "Disable SHAP explanations because of small number of samples (< 20).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_Default_RandomForest rmse 38041.992782 trained in 0.63 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble rmse 2492.139724 trained in 0.14 seconds\n",
      "AutoML fit time: 8.15 seconds\n",
      "AutoML best model: Ensemble\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = automl.predict_all(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.144057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>182.673882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14156.072703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173.155802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45771.711119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction\n",
       "0    173.144057\n",
       "1    182.673882\n",
       "2  14156.072703\n",
       "3    173.155802\n",
       "4  45771.711119"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted value')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, pred.prediction, '.')\n",
    "plt.xlabel(\"True value\")\n",
    "plt.ylabel(\"Predicted value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total   NaN\n",
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "5       NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Absolute Error on test data\n",
    "np.mean(np.abs(y_test-pred.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_9\n",
      "The task is multiclass_classification with evaluation metric logloss\n",
      "AutoML will use algorithms: ['Linear', 'Random Forest', 'LightGBM', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 1 model\n",
      "1_Linear logloss 0.11709 trained in 3.15 seconds (1-sample predict time 0.0515 seconds)\n",
      "* Step default_algorithms will try to check up to 4 models\n",
      "2_Default_LightGBM logloss 0.119508 trained in 57.1 seconds (1-sample predict time 0.0555 seconds)\n",
      "3_Default_Xgboost logloss 0.141544 trained in 34.88 seconds (1-sample predict time 0.049 seconds)\n",
      "4_Default_NeuralNetwork logloss 0.244656 trained in 4.06 seconds (1-sample predict time 0.0518 seconds)\n",
      "5_Default_RandomForest logloss 0.835827 trained in 10.88 seconds (1-sample predict time 0.1076 seconds)\n",
      "* Step not_so_random will try to check up to 16 models\n",
      "10_LightGBM logloss 0.108165 trained in 25.7 seconds (1-sample predict time 0.0542 seconds)\n",
      "6_Xgboost logloss 0.167262 trained in 54.14 seconds (1-sample predict time 0.0507 seconds)\n",
      "14_RandomForest logloss 0.807765 trained in 10.22 seconds (1-sample predict time 0.1066 seconds)\n",
      "18_NeuralNetwork logloss 0.438477 trained in 6.41 seconds (1-sample predict time 0.0696 seconds)\n",
      "11_LightGBM logloss 0.094231 trained in 15.62 seconds (1-sample predict time 0.0465 seconds)\n",
      "7_Xgboost logloss 0.141853 trained in 43.39 seconds (1-sample predict time 0.0615 seconds)\n",
      "15_RandomForest logloss 0.447175 trained in 16.25 seconds (1-sample predict time 0.1427 seconds)\n",
      "19_NeuralNetwork logloss 0.15857 trained in 5.54 seconds (1-sample predict time 0.055 seconds)\n",
      "12_LightGBM logloss 0.112702 trained in 522.43 seconds (1-sample predict time 0.049 seconds)\n",
      "8_Xgboost logloss 0.312174 trained in 37.47 seconds (1-sample predict time 0.049 seconds)\n",
      "16_RandomForest logloss 1.104381 trained in 8.06 seconds (1-sample predict time 0.0873 seconds)\n",
      "20_NeuralNetwork logloss 0.404874 trained in 5.59 seconds (1-sample predict time 0.0524 seconds)\n",
      "13_LightGBM logloss 0.132 trained in 75.03 seconds (1-sample predict time 0.0545 seconds)\n",
      "9_Xgboost logloss 0.877835 trained in 33.07 seconds (1-sample predict time 0.0475 seconds)\n",
      "17_RandomForest logloss 0.505358 trained in 12.47 seconds (1-sample predict time 0.1094 seconds)\n",
      "21_NeuralNetwork logloss 0.358318 trained in 5.85 seconds (1-sample predict time 0.0582 seconds)\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 10\n",
      "Add Golden Feature: 36_multiply_28\n",
      "Add Golden Feature: 18_ratio_26\n",
      "Add Golden Feature: 36_multiply_4\n",
      "Add Golden Feature: 52_ratio_36\n",
      "Add Golden Feature: 57_sum_2\n",
      "Add Golden Feature: 61_ratio_3\n",
      "Add Golden Feature: 59_ratio_2\n",
      "Add Golden Feature: 61_ratio_34\n",
      "Add Golden Feature: 13_ratio_2\n",
      "Add Golden Feature: 2_ratio_13\n",
      "Created 10 Golden Features in 1.72 seconds.\n",
      "11_LightGBM_GoldenFeatures logloss 0.093966 trained in 17.8 seconds (1-sample predict time 0.0645 seconds)\n",
      "10_LightGBM_GoldenFeatures logloss 0.108449 trained in 32.23 seconds (1-sample predict time 0.0695 seconds)\n",
      "1_Linear_GoldenFeatures logloss 0.124872 trained in 5.36 seconds (1-sample predict time 0.0957 seconds)\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "11_LightGBM_GoldenFeatures_RandomFeature logloss 0.101164 trained in 17.26 seconds (1-sample predict time 0.0665 seconds)\n",
      "Drop features ['14', '36_multiply_4', '49', '7', '36_multiply_28', '23', '1', '47', '24', '48', '56', '8', '16', '31', '40', '15', 'random_feature', '11', '61_ratio_3', '36']\n",
      "* Step features_selection will try to check up to 4 models\n",
      "11_LightGBM_GoldenFeatures_SelectedFeatures logloss 0.103646 trained in 15.28 seconds (1-sample predict time 0.0571 seconds)\n",
      "3_Default_Xgboost_SelectedFeatures logloss 0.141114 trained in 43.86 seconds (1-sample predict time 0.0465 seconds)\n",
      "19_NeuralNetwork_SelectedFeatures logloss 0.153551 trained in 6.71 seconds (1-sample predict time 0.0451 seconds)\n",
      "15_RandomForest_SelectedFeatures logloss 0.44574 trained in 14.3 seconds (1-sample predict time 0.1091 seconds)\n",
      "* Step hill_climbing_1 will try to check up to 10 models\n",
      "22_LightGBM_GoldenFeatures logloss 0.096235 trained in 30.6 seconds (1-sample predict time 0.0676 seconds)\n",
      "23_LightGBM logloss 0.093545 trained in 29.99 seconds (1-sample predict time 0.047 seconds)\n",
      "24_Xgboost_SelectedFeatures logloss 0.136842 trained in 43.98 seconds (1-sample predict time 0.042 seconds)\n",
      "25_Xgboost_SelectedFeatures logloss 0.139399 trained in 53.37 seconds (1-sample predict time 0.0435 seconds)\n",
      "26_Xgboost logloss 0.138523 trained in 67.4 seconds (1-sample predict time 0.0515 seconds)\n",
      "27_Xgboost logloss 0.141792 trained in 57.16 seconds (1-sample predict time 0.0511 seconds)\n",
      "28_NeuralNetwork_SelectedFeatures logloss 0.18616 trained in 7.39 seconds (1-sample predict time 0.0475 seconds)\n",
      "29_NeuralNetwork logloss 0.164307 trained in 8.15 seconds (1-sample predict time 0.056 seconds)\n",
      "30_RandomForest_SelectedFeatures logloss 0.414753 trained in 13.55 seconds (1-sample predict time 0.1062 seconds)\n",
      "31_RandomForest logloss 0.409467 trained in 19.31 seconds (1-sample predict time 0.1493 seconds)\n",
      "* Step hill_climbing_2 will try to check up to 9 models\n",
      "32_LightGBM logloss 0.086937 trained in 54.67 seconds (1-sample predict time 0.051 seconds)\n",
      "33_Xgboost_SelectedFeatures logloss 0.128594 trained in 53.73 seconds (1-sample predict time 0.0471 seconds)\n",
      "34_Xgboost logloss 0.13254 trained in 61.17 seconds (1-sample predict time 0.049 seconds)\n",
      "35_NeuralNetwork_SelectedFeatures logloss 0.295979 trained in 8.75 seconds (1-sample predict time 0.0484 seconds)\n",
      "36_NeuralNetwork logloss 0.300124 trained in 9.41 seconds (1-sample predict time 0.056 seconds)\n",
      "37_RandomForest logloss 0.365933 trained in 22.06 seconds (1-sample predict time 0.1695 seconds)\n",
      "38_RandomForest logloss 0.444465 trained in 23.09 seconds (1-sample predict time 0.1444 seconds)\n",
      "39_RandomForest_SelectedFeatures logloss 0.372102 trained in 14.03 seconds (1-sample predict time 0.1239 seconds)\n",
      "40_RandomForest_SelectedFeatures logloss 0.450868 trained in 13.11 seconds (1-sample predict time 0.0972 seconds)\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble logloss 0.072639 trained in 4.51 seconds (1-sample predict time 0.6096 seconds)\n",
      "AutoML fit time: 1802.67 seconds\n",
      "AutoML best model: 32_LightGBM\n",
      "   prediction_0  prediction_1  prediction_2  prediction_3  prediction_4  \\\n",
      "0      0.001350      0.001354      0.002451      0.037585      0.001292   \n",
      "1      0.000076      0.000786      0.000139      0.000819      0.000210   \n",
      "2      0.000017      0.000019      0.000027      0.999463      0.000017   \n",
      "3      0.000020      0.000164      0.000022      0.000032      0.004301   \n",
      "4      0.000214      0.000390      0.000170      0.000320      0.000396   \n",
      "\n",
      "   prediction_5  prediction_6  prediction_7  prediction_8  prediction_9  label  \n",
      "0      0.001652      0.002664      0.001298      0.227644      0.722710      9  \n",
      "1      0.000798      0.000164      0.000062      0.996589      0.000358      8  \n",
      "2      0.000025      0.000018      0.000020      0.000054      0.000339      3  \n",
      "3      0.000061      0.000029      0.995320      0.000031      0.000021      7  \n",
      "4      0.000287      0.000157      0.996038      0.000448      0.001581      7  \n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "csp = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pd.DataFrame(csp.data), csp.target, stratify=csp.target, test_size=0.25,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# train models with AutoML\n",
    "automl = AutoML(mode=\"Perform\")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "# compute the accuracy on test data\n",
    "predictions = automl.predict_all(X_test)\n",
    "print(predictions.head())\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, predictions[\"label\"].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_10\n",
      "The task is multiclass_classification with evaluation metric logloss\n",
      "AutoML will use algorithms: ['Linear', 'Random Forest', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 1 model\n",
      "1_Linear logloss 1.238823 trained in 5.31 seconds (1-sample predict time 0.0258 seconds)\n",
      "* Step default_algorithms will try to check up to 5 models\n",
      "2_Default_LightGBM logloss 8.2e-05 trained in 13.22 seconds (1-sample predict time 0.021 seconds)\n",
      "3_Default_Xgboost logloss 0.039463 trained in 15.84 seconds (1-sample predict time 0.025 seconds)\n",
      "4_Default_CatBoost logloss 0.000228 trained in 79.71 seconds (1-sample predict time 0.0213 seconds)\n",
      "5_Default_NeuralNetwork logloss 0.944295 trained in 3.03 seconds (1-sample predict time 0.0265 seconds)\n",
      "6_Default_RandomForest logloss 1.112559 trained in 4.49 seconds (1-sample predict time 0.0671 seconds)\n",
      "* Step not_so_random will try to check up to 20 models\n",
      "11_LightGBM logloss 7.5e-05 trained in 12.95 seconds (1-sample predict time 0.0224 seconds)\n",
      "7_Xgboost logloss 0.233973 trained in 18.96 seconds (1-sample predict time 0.022 seconds)\n",
      "15_CatBoost logloss 0.000281 trained in 51.16 seconds (1-sample predict time 0.0195 seconds)\n",
      "19_RandomForest logloss 0.891376 trained in 4.85 seconds (1-sample predict time 0.0581 seconds)\n",
      "23_NeuralNetwork logloss 1.119514 trained in 3.94 seconds (1-sample predict time 0.0245 seconds)\n",
      "12_LightGBM logloss 7.2e-05 trained in 4.89 seconds (1-sample predict time 0.0201 seconds)\n",
      "8_Xgboost logloss 0.039444 trained in 14.25 seconds (1-sample predict time 0.021 seconds)\n",
      "16_CatBoost logloss 0.000148 trained in 78.12 seconds (1-sample predict time 0.0183 seconds)\n",
      "20_RandomForest logloss 0.649932 trained in 30.64 seconds (1-sample predict time 0.053 seconds)\n",
      "24_NeuralNetwork logloss 0.997457 trained in 6.19 seconds (1-sample predict time 0.0371 seconds)\n",
      "13_LightGBM logloss 8.2e-05 trained in 15.79 seconds (1-sample predict time 0.02 seconds)\n",
      "9_Xgboost logloss 2.995741 trained in 6.23 seconds (1-sample predict time 0.0415 seconds)\n",
      "17_CatBoost logloss 0.000131 trained in 118.11 seconds (1-sample predict time 0.02 seconds)\n",
      "21_RandomForest logloss 1.487978 trained in 7.59 seconds (1-sample predict time 0.0731 seconds)\n",
      "25_NeuralNetwork logloss 0.923888 trained in 6.01 seconds (1-sample predict time 0.0253 seconds)\n",
      "14_LightGBM logloss 8.1e-05 trained in 12.34 seconds (1-sample predict time 0.021 seconds)\n",
      "10_Xgboost logloss 2.995732 trained in 7.15 seconds (1-sample predict time 0.022 seconds)\n",
      "18_CatBoost logloss 0.000175 trained in 112.87 seconds (1-sample predict time 0.02 seconds)\n",
      "22_RandomForest logloss 1.083685 trained in 7.65 seconds (1-sample predict time 0.0592 seconds)\n",
      "26_NeuralNetwork logloss 0.886157 trained in 6.78 seconds (1-sample predict time 0.0244 seconds)\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 10\n",
      "Add Golden Feature: Centers for Medicare and Medicaid Services_diff_samhsa\n",
      "Add Golden Feature: hrsa_multiply_Centers for Disease Control and Prevention\n",
      "Add Golden Feature: aspr_sum_acf\n",
      "Add Golden Feature: acf_ratio_Centers for Medicare and Medicaid Services\n",
      "Add Golden Feature: samhsa_ratio_acf\n",
      "Add Golden Feature: Centers for Medicare and Medicaid Services_sum_ahrq\n",
      "Add Golden Feature: aspr_ratio_Food and Drug Administration\n",
      "Add Golden Feature: aspr_ratio_os\n",
      "Add Golden Feature: Centers for Medicare and Medicaid Services_ratio_os\n",
      "Add Golden Feature: acf_ratio_samhsa\n",
      "Created 10 Golden Features in 0.08 seconds.\n",
      "12_LightGBM_GoldenFeatures logloss 5.9e-05 trained in 8.39 seconds (1-sample predict time 0.0371 seconds)\n",
      "11_LightGBM_GoldenFeatures logloss 6.5e-05 trained in 21.49 seconds (1-sample predict time 0.039 seconds)\n",
      "14_LightGBM_GoldenFeatures logloss 6.5e-05 trained in 20.35 seconds (1-sample predict time 0.0465 seconds)\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "12_LightGBM_GoldenFeatures_RandomFeature logloss 6.6e-05 trained in 15.03 seconds (1-sample predict time 0.038 seconds)\n",
      "Drop features ['random_feature']\n",
      "Skip features_selection because no parameters were generated.\n",
      "* Step hill_climbing_1 will try to check up to 16 models\n",
      "27_LightGBM_GoldenFeatures logloss 5.9e-05 trained in 9.83 seconds (1-sample predict time 0.0401 seconds)\n",
      "28_LightGBM_GoldenFeatures logloss 6.8e-05 trained in 22.69 seconds (1-sample predict time 0.0395 seconds)\n",
      "29_LightGBM_GoldenFeatures logloss 6.5e-05 trained in 22.39 seconds (1-sample predict time 0.0405 seconds)\n",
      "30_CatBoost logloss 0.000129 trained in 130.08 seconds (1-sample predict time 0.0189 seconds)\n",
      "31_CatBoost logloss 0.000131 trained in 150.74 seconds (1-sample predict time 0.02 seconds)\n",
      "32_CatBoost logloss 0.000148 trained in 101.73 seconds (1-sample predict time 0.0192 seconds)\n",
      "33_Xgboost logloss 0.231766 trained in 20.53 seconds (1-sample predict time 0.021 seconds)\n",
      "34_Xgboost logloss 0.233973 trained in 23.87 seconds (1-sample predict time 0.022 seconds)\n",
      "35_RandomForest logloss 0.391698 trained in 10.66 seconds (1-sample predict time 0.0657 seconds)\n",
      "36_RandomForest logloss 0.823128 trained in 11.36 seconds (1-sample predict time 0.0718 seconds)\n",
      "37_NeuralNetwork logloss 0.757273 trained in 10.12 seconds (1-sample predict time 0.0241 seconds)\n",
      "38_NeuralNetwork logloss 0.958797 trained in 9.59 seconds (1-sample predict time 0.0287 seconds)\n",
      "39_RandomForest logloss 0.865506 trained in 10.99 seconds (1-sample predict time 0.0626 seconds)\n",
      "40_RandomForest logloss 1.074952 trained in 11.65 seconds (1-sample predict time 0.0637 seconds)\n",
      "41_NeuralNetwork logloss 0.83291 trained in 10.68 seconds (1-sample predict time 0.0251 seconds)\n",
      "42_NeuralNetwork logloss 1.01928 trained in 10.6 seconds (1-sample predict time 0.025 seconds)\n",
      "* Step hill_climbing_2 will try to check up to 15 models\n",
      "43_LightGBM_GoldenFeatures logloss 5.9e-05 trained in 11.5 seconds (1-sample predict time 0.037 seconds)\n",
      "44_LightGBM_GoldenFeatures logloss 5.9e-05 trained in 11.79 seconds (1-sample predict time 0.036 seconds)\n",
      "45_LightGBM_GoldenFeatures logloss 5.9e-05 trained in 12.34 seconds (1-sample predict time 0.035 seconds)\n",
      "46_LightGBM_GoldenFeatures logloss 5.9e-05 trained in 12.36 seconds (1-sample predict time 0.036 seconds)\n",
      "47_CatBoost logloss 0.00015 trained in 100.81 seconds (1-sample predict time 0.0197 seconds)\n",
      "48_Xgboost logloss 0.039463 trained in 23.69 seconds (1-sample predict time 0.021 seconds)\n",
      "49_Xgboost logloss 0.038921 trained in 17.53 seconds (1-sample predict time 0.02 seconds)\n",
      "50_Xgboost logloss 0.039648 trained in 29.51 seconds (1-sample predict time 0.023 seconds)\n",
      "51_Xgboost logloss 0.039444 trained in 24.53 seconds (1-sample predict time 0.022 seconds)\n",
      "52_RandomForest logloss 0.335999 trained in 15.75 seconds (1-sample predict time 0.0786 seconds)\n",
      "53_RandomForest logloss 0.403412 trained in 14.77 seconds (1-sample predict time 0.0697 seconds)\n",
      "54_RandomForest logloss 0.620494 trained in 14.24 seconds (1-sample predict time 0.0631 seconds)\n",
      "55_RandomForest logloss 0.638132 trained in 14.15 seconds (1-sample predict time 0.0597 seconds)\n",
      "56_NeuralNetwork logloss 1.062015 trained in 13.38 seconds (1-sample predict time 0.025 seconds)\n",
      "57_NeuralNetwork logloss 0.813494 trained in 13.74 seconds (1-sample predict time 0.0281 seconds)\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble logloss 5.9e-05 trained in 8.27 seconds (1-sample predict time 0.0749 seconds)\n",
      "AutoML fit time: 1741.68 seconds\n",
      "AutoML best model: Ensemble\n",
      "   prediction_0.12  prediction_0.403  prediction_0.597  prediction_15.0  \\\n",
      "0         0.027546          0.022837          0.346139         0.049173   \n",
      "1         0.060586          0.053569          0.031693         0.018157   \n",
      "2         0.025044          0.090733          0.014815         0.252642   \n",
      "3         0.000263          0.000215          0.000086         0.000045   \n",
      "4         0.745998          0.000034          0.027008         0.001633   \n",
      "\n",
      "   prediction_62.0  prediction_79.0  prediction_458.0  prediction_758.0  \\\n",
      "0         0.026012         0.010256          0.027094          0.018789   \n",
      "1         0.025320         0.057344          0.075871          0.066494   \n",
      "2         0.013946         0.016342          0.172123          0.027159   \n",
      "3         0.954629         0.000357          0.034424          0.000070   \n",
      "4         0.000005         0.000002          0.000004          0.000004   \n",
      "\n",
      "   prediction_805.0  prediction_3173.0  ...  prediction_5381.0  \\\n",
      "0          0.026424           0.034113  ...           0.015222   \n",
      "1          0.056416           0.054522  ...           0.032484   \n",
      "2          0.078058           0.022197  ...           0.016965   \n",
      "3          0.000614           0.000215  ...           0.001279   \n",
      "4          0.000005           0.000006  ...           0.000020   \n",
      "\n",
      "   prediction_10237.0  prediction_11991.0  prediction_16429.0  \\\n",
      "0            0.078069            0.018058            0.167345   \n",
      "1            0.028340            0.038034            0.094042   \n",
      "2            0.039081            0.020420            0.073700   \n",
      "3            0.000051            0.003993            0.000080   \n",
      "4            0.000063            0.000004            0.000051   \n",
      "\n",
      "   prediction_18358.0  prediction_35074.0  prediction_51901.0  \\\n",
      "0            0.028949            0.012456            0.025457   \n",
      "1            0.064015            0.026485            0.061356   \n",
      "2            0.025136            0.019539            0.025068   \n",
      "3            0.000240            0.000246            0.000211   \n",
      "4            0.225147            0.000003            0.000004   \n",
      "\n",
      "   prediction_76738.0  prediction_86975.0      label  \n",
      "0            0.025646            0.025653      0.597  \n",
      "1            0.062279            0.062299  16429.000  \n",
      "2            0.025090            0.025081     15.000  \n",
      "3            0.000220            0.000221     62.000  \n",
      "4            0.000004            0.000004      0.120  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23540\\2901043501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multilabel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m     95\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "csp = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y, test_size=0.25,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# train models with AutoML\n",
    "automl = AutoML(mode=\"Perform\")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "# compute the accuracy on test data\n",
    "predictions = automl.predict_all(X_test)\n",
    "print(predictions.head())\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
